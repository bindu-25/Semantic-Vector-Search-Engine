# -*- coding: utf-8 -*-
"""run_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oaFDXVMuwAABPdN94uPyjsZl6X8QS72F
"""

from pmc_client import search_pmc, fetch_pmc_xml, extract_sections

def load_documents(max_papers=10, min_len=300):
    """
    Build a small, fixed corpus for evaluation.
    This is ONLY for benchmarking, not production.
    """
    pmcids = search_pmc(
        query="semantic vector search embeddings",
        max_papers=max_papers
    )

    documents = []

    for pmcid in pmcids:
        root = fetch_pmc_xml(pmcid)
        if root is None:
            continue

        for _, text in extract_sections(root):
            if text and len(text) >= min_len:
                documents.append(text)

    print(f"Loaded {len(documents)} documents for evaluation")
    return documents


documents = load_documents()

documents[:1][0][:500]

import time
import numpy as np
from baseline_tfidf import TfidfSearch
from semantic_search_engine import SemanticSearchEngine

queries = [
    "semantic document retrieval",
    "transformer embeddings",
    "vector similarity search",
    "meaning based ranking",
    "cosine similarity in NLP"
]

# Initialize engines
tfidf_engine = TfidfSearch(documents)
semantic_engine = SemanticSearchEngine()

# Manual relevance labels (simple + illustrative)
# Pick 2â€“3 docs per query after inspecting results once
relevance = {
    0: {0, 1},
    1: {2, 3},
    2: {1, 4},
    3: {0, 2},
    4: {3, 4},
}

def precision_at_k(results, relevant, k=5):
    return len(set(results[:k]) & relevant) / k

tfidf_scores = []
semantic_scores = []

for i, q in enumerate(queries):
    # TF-IDF
    tfidf_results = tfidf_engine.search(q, top_k=5)
    tfidf_scores.append(
        precision_at_k(tfidf_results, relevance[i])
    )

    # Semantic
    doc_embeddings = semantic_engine.embed_texts(documents)
    query_embedding = semantic_engine.embed_texts([q])[0]
    scores = doc_embeddings @ query_embedding
    semantic_results = np.argsort(scores)[::-1][:5]

    semantic_scores.append(
        precision_at_k(semantic_results, relevance[i])
    )

print("TF-IDF Precision@5:", np.mean(tfidf_scores))
print("Semantic Precision@5:", np.mean(semantic_scores))

import time
import numpy as np

queries = [
    "semantic document retrieval",
    "vector search engine",
    "transformer embeddings",
] * 10

latencies = []

for q in queries:
    start = time.time()
    _ = semantic_engine.embed_texts([q])
    latencies.append(time.time() - start)

print("Average embedding latency:", np.mean(latencies), "seconds")